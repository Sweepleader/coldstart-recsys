import torch
import torch.nn as nn
from PIL import Image
from torchvision import transforms
import os
import csv
import glob
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score

from multimodal_encoder_1216 import (
    VideoEncoder, 
    RobustAudioEncoder, 
    RobustTextEncoder, 
    MultiModalAttentionFusion,
    M3CSR_MultiModalEncoder
)

# ------------------------------------------------------------------------------
# ç¨³å®šæ€§è®¾ç½®
# ------------------------------------------------------------------------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# ------------------------------------------------------------------------------
# å¯è§†åŒ–å‡½æ•°
# ------------------------------------------------------------------------------
def visualize_clusters(features, cluster_ids, item_ids, raw_titles, save_path="cluster_viz.png"):
    """
    ä½¿ç”¨ PCA å°†ç‰¹å¾é™ç»´åˆ° 2D å¹¶ç»˜åˆ¶æ•£ç‚¹å›¾ã€‚
    features: [N, dim]
    cluster_ids: [N]
    """
    # è½¬æ¢ä¸º numpy
    if isinstance(features, torch.Tensor):
        X = features.cpu().numpy()
    else:
        X = features
        
    if isinstance(cluster_ids, torch.Tensor):
        c_ids = cluster_ids.cpu().numpy()
    else:
        c_ids = cluster_ids
        
    # PCA é™ç»´
    pca = PCA(n_components=2)
    X_2d = pca.fit_transform(X)
    
    plt.figure(figsize=(12, 10))
    
    # å®šä¹‰é¢œè‰²æ˜ å°„
    # å‡è®¾ k=3, æˆ‘ä»¬ç”¨ distinct colors
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    # ç»˜åˆ¶æ•£ç‚¹
    unique_clusters = np.unique(c_ids)
    for cid in unique_clusters:
        mask = (c_ids == cid)
        plt.scatter(
            X_2d[mask, 0], X_2d[mask, 1], 
            label=f'Cluster {cid}', 
            s=200, 
            alpha=0.8,
            edgecolors='w', 
            linewidth=2
        )
        
    # æ·»åŠ æ ‡ç­¾
    for i, uid in enumerate(item_ids):
        x, y = X_2d[i, 0], X_2d[i, 1]
        
        # å¤„ç†æ ‡é¢˜ï¼Œé˜²æ­¢è¿‡é•¿
        title = raw_titles[i]
        # ç®€å•çš„æˆªæ–­é€»è¾‘
        display_title = f"ID:{uid}\n"
        if "#" in title:
            # ä¼˜å…ˆæ˜¾ç¤º tag
            tags = [t for t in title.split() if t.startswith("#")]
            display_title += " ".join(tags[:2])
        else:
            display_title += title[:15] + "..."
            
        plt.annotate(
            display_title, 
            (x, y), 
            xytext=(10, 10), 
            textcoords='offset points',
            fontsize=9,
            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8)
        )
        
    plt.title("M3CSR Multi-modal Feature Clustering (PCA 2D Projection)", fontsize=16)
    plt.xlabel(f"PCA Component 1 (Var: {pca.explained_variance_ratio_[0]:.2f})")
    plt.ylabel(f"PCA Component 2 (Var: {pca.explained_variance_ratio_[1]:.2f})")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend()
    
    # ä¿å­˜
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"\n[Visualization] èšç±»åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")
    plt.close()

# ------------------------------------------------------------------------------
# æ•°æ®åŠ è½½è¾…åŠ©å‡½æ•°
# ------------------------------------------------------------------------------

def load_video_frames_for_id(base_dir, item_id, num_frames=5):
    """
    ä¸ºæŒ‡å®š ID åŠ è½½è§†é¢‘å¸§ã€‚
    åœ¨ base_dir ä¸­å¯»æ‰¾ {id}-{seq}.png æˆ– {id}-{seq}.jpg
    è¿”å›: [1, T, 3, H, W]
    """
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    
    frames = []
    # å°è¯•æ‰¾åˆ°å¯¹åº”çš„å¸§æ–‡ä»¶
    # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º: item_id-seq.ext (ä¾‹å¦‚ 1-1.png)
    
    for i in range(1, num_frames + 1):
        # å°è¯• png å’Œ jpg
        found = False
        for ext in ['png', 'jpg', 'jpeg']:
            fname = f"{item_id}-{i}.{ext}"
            fpath = os.path.join(base_dir, fname)
            if os.path.exists(fpath):
                try:
                    img = Image.open(fpath).convert('RGB')
                    frames.append(transform(img))
                    found = True
                    break
                except Exception as e:
                    print(f"  [Error] åŠ è½½å¸§ {fpath} å¤±è´¥: {e}")
        
        if not found:
            # å¦‚æœä¸­é—´æŸå¸§ç¼ºå¤±ï¼Œç”¨å…¨é»‘å¡«å……ï¼Œæˆ–è€…å¤ç”¨ä¸Šä¸€å¸§
            # è¿™é‡Œç®€å•ç”¨å…¨é»‘
            frames.append(torch.zeros(3, 224, 224))
            
    if not frames:
        return torch.randn(1, num_frames, 3, 224, 224)
        
    return torch.stack(frames).unsqueeze(0)

def load_cover_for_id(base_dir, item_id):
    """
    åŠ è½½å°é¢: base_dir/{item_id}.jpg
    """
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    
    # å°è¯•å¤šç§æ‰©å±•å
    for ext in ['jpg', 'png', 'jpeg']:
        path = os.path.join(base_dir, f"{item_id}.{ext}")
        if os.path.exists(path):
            try:
                img = Image.open(path).convert('RGB')
                return transform(img).unsqueeze(0)
            except:
                pass
                
    # é»˜è®¤é»‘è‰²
    return torch.zeros(1, 3, 224, 224)

def load_audio_for_id(base_dir, item_id):
    """
    åŠ è½½éŸ³é¢‘: base_dir/{item_id}.wav
    """
    path = os.path.join(base_dir, f"{item_id}.wav")
    
    # å¦‚æœæ²¡æœ‰ wavï¼Œå°è¯•æŸ¥æ‰¾ mp4 å¹¶æå– (è¿™é‡Œä»…åšè·¯å¾„æ£€æŸ¥ï¼Œä¸å®é™…åšå¤æ‚çš„ ffmpeg æå–ï¼Œé™¤éå¿…è¦)
    # ä¸ºç®€å•èµ·è§ï¼Œå¦‚æœæ‰¾ä¸åˆ° wavï¼Œå›é€€åˆ°éšæœº/é™éŸ³
    
    if os.path.exists(path):
        try:
            import torchaudio
            import torchaudio.transforms as T
            
            # æŠ‘åˆ¶ torchaudio è­¦å‘Š
            import warnings
            warnings.filterwarnings("ignore")

            waveform, sample_rate = torchaudio.load(path)
            
            if sample_rate != 16000:
                resampler = T.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
            
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
                
            mel_transform = T.MelSpectrogram(sample_rate=16000, n_mels=64)
            mel_spec = mel_transform(waveform)
            
            target_time = 100
            current_time = mel_spec.shape[2]
            
            if current_time > target_time:
                mel_spec = mel_spec[:, :, :target_time]
            elif current_time < target_time:
                pad_amount = target_time - current_time
                pad = torch.zeros(1, 64, pad_amount)
                mel_spec = torch.cat([mel_spec, pad], dim=2)
                
            return mel_spec.unsqueeze(0) # [1, 1, 64, 100]
            
        except Exception as e:
            print(f"  [Warning] éŸ³é¢‘åŠ è½½å¤±è´¥ {path}: {e}")
            
    # éšæœºå™ªå£°å›é€€
    return torch.randn(1, 1, 64, 100)

def load_titles(csv_path):
    """
    è¯»å– titles.csv è¿”å› {id: title}
    """
    titles = {}
    if not os.path.exists(csv_path):
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ° titles.csv: {csv_path}")
        return titles
        
    try:
        # ä½¿ç”¨ utf-8-sig å¤„ç†å¯èƒ½çš„ BOM
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            # è§„èŒƒåŒ– header: å»é™¤ç©ºæ ¼
            if reader.fieldnames:
                reader.fieldnames = [name.strip() for name in reader.fieldnames]
                
            for row in reader:
                # å†æ¬¡æ¸…ç† row keys (ä»¥é˜²ä¸‡ä¸€)
                clean_row = {k.strip(): v for k, v in row.items() if k}
                
                if 'item' in clean_row and 'title' in clean_row:
                    try:
                        uid = int(clean_row['item'])
                        titles[uid] = clean_row['title']
                    except ValueError:
                        pass
    except Exception as e:
        print(f"è¯»å– titles.csv å¤±è´¥: {e}")
        
    return titles

# ------------------------------------------------------------------------------
# èšç±»ç®—æ³• (è‡ªåŠ¨é€‰æ‹©æœ€ä½³ K)
# ------------------------------------------------------------------------------
def perform_kmeans_auto(features, seed=42):
    """
    è‡ªåŠ¨é€‰æ‹©æœ€ä½³ K å€¼çš„ K-Means èšç±» (åŸºäº Silhouette Score)
    features: [N, dim]
    è¿”å›: labels [N], best_k
    """
    # 1. è½¬æ¢ä¸º Numpy
    if isinstance(features, torch.Tensor):
        X = features.cpu().numpy()
    else:
        X = features
        
    # 2. L2 å½’ä¸€åŒ–
    X_norm = normalize(X, norm='l2')
    
    N = X_norm.shape[0]
    
    # 3. éå†å¯»æ‰¾æœ€ä½³ K
    best_score = -1.0
    best_k = 2
    best_labels = None
    
    # K çš„èŒƒå›´ï¼šä» 2 åˆ° N-1 (Silhouette Score éœ€è¦è‡³å°‘ 2 ä¸ªç°‡ï¼Œä¸”ä¸èƒ½æ¯ä¸ªæ ·æœ¬ä¸€ç°‡)
    max_k = min(N - 1, 6) # å¯¹äºåªæœ‰7ä¸ªæ ·æœ¬ï¼Œå°è¯•åˆ° 6 å³å¯
    
    if max_k < 2:
        # æ ·æœ¬å¤ªå°‘ï¼Œå¼ºåˆ¶åˆ†ä¸º 2 ç±»æˆ–ç›´æ¥è¿”å›å…¨ 0
        print("æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è‡ªåŠ¨é€‰æ‹© Kï¼Œé»˜è®¤ k=2")
        kmeans = KMeans(n_clusters=2, random_state=seed, n_init=50)
        labels = kmeans.fit_predict(X_norm)
        return torch.tensor(labels, dtype=torch.long), 2
        
    print(f"è‡ªåŠ¨å¯»æ‰¾æœ€ä½³ K å€¼ (èŒƒå›´ 2-{max_k})...")
    
    for k in range(2, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=seed, n_init=50)
        labels = kmeans.fit_predict(X_norm)
        
        score = silhouette_score(X_norm, labels)
        print(f"  k={k}: Silhouette Score = {score:.4f}")
        
        if score > best_score:
            best_score = score
            best_k = k
            best_labels = labels
            
    print(f"âœ… é€‰å®šæœ€ä½³ K å€¼: {best_k} (Score: {best_score:.4f})")
    
    return torch.tensor(best_labels, dtype=torch.long), best_k

# ------------------------------------------------------------------------------
# ä¸»æµç¨‹
# ------------------------------------------------------------------------------
def main():
    # å›ºå®šéšæœºç§å­
    set_seed(42)
    
    print("="*60)
    print("M3CSR å¤šæ¨¡æ€å¤„ç†æµç¨‹ (Batch Processing - Auto Clustering)")
    print("="*60)
    
    # 1. è·¯å¾„é…ç½®
    base_dir = "f:\\coldstart-recsys\\models\\mutibehavior\\test_file"
    frame_dir = os.path.join(base_dir, "test_frames_interval_1_number_5")
    cover_dir = os.path.join(base_dir, "test_covers")
    title_path = os.path.join(base_dir, "titles.csv")
    
    # 2. å‡†å¤‡æ•°æ® ID åˆ—è¡¨ (1-7)
    item_ids = list(range(1, 8))
    
    # åŠ è½½æ ‡é¢˜æ˜ å°„
    title_map = load_titles(title_path)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    print("\n[åˆå§‹åŒ–æ¨¡å‹]")
    num_clusters = 5 # å‡è®¾èšæˆ 5 ç±» (æ ·æœ¬å°‘ï¼Œè®¾å°ä¸€ç‚¹)
    model = M3CSR_MultiModalEncoder(num_clusters=num_clusters, unified_dim=256)
    model.eval()
    
    # 4. æ”¶é›†æ•°æ® & é˜¶æ®µ 1: åŸºç¡€ç‰¹å¾æå–
    print("\n[é˜¶æ®µ 1: æ•°æ®åŠ è½½ä¸åŸºç¡€ç‰¹å¾æå–]")
    
    batch_data = [] # å­˜å‚¨ (video, audio, text, cover, item_id)
    base_embeddings = []
    
    for uid in item_ids:
        print(f"æ­£åœ¨å¤„ç† Item ID: {uid} ...")
        
        # åŠ è½½å„æ¨¡æ€
        vid_input = load_video_frames_for_id(frame_dir, uid)
        aud_input = load_audio_for_id(base_dir, uid) # å‡è®¾ wav åœ¨ test_file æ ¹ç›®å½•
        cov_input = load_cover_for_id(cover_dir, uid)
        
        # æ–‡æœ¬
        raw_text = title_map.get(uid, "Unknown content")
        txt_input = [raw_text]
        
        # å­˜å‚¨è¾“å…¥ä»¥ä¾¿é˜¶æ®µ 3 ä½¿ç”¨
        batch_data.append({
            'id': uid,
            'video': vid_input,
            'audio': aud_input,
            'cover': cov_input,
            'text': txt_input,
            'raw_text': raw_text
        })
        
        # æ¨ç† (ä¸å¸¦ ID)
        with torch.no_grad():
            # è¿”å›: fused, weights, bases
            base_fused, _, _ = model(vid_input, aud_input, txt_input, cluster_ids=None, cover=cov_input)
            base_embeddings.append(base_fused)
            
    # å †å æ‰€æœ‰åŸºç¡€ç‰¹å¾ [N, dim]
    all_base_features = torch.cat(base_embeddings, dim=0) # [7, 128]
    print(f"\næ‰€æœ‰ Item åŸºç¡€ç‰¹å¾æå–å®Œæ¯•: {all_base_features.shape}")
    
    # 5. é˜¶æ®µ 2: å…¨å±€èšç±» (Global Clustering)
    print("\n[é˜¶æ®µ 2: å…¨å±€è¯­ä¹‰èšç±» (æ¨¡æ‹Ÿ Offline è¿‡ç¨‹)]")
    # ä½¿ç”¨ K-Means å°† items åˆ†ç»„
    
    # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ K
    cluster_assignments, best_k = perform_kmeans_auto(all_base_features) 
    print("èšç±»ç»“æœ:")
    
    # æ”¶é›†æ ‡é¢˜ç”¨äºå¯è§†åŒ–
    all_raw_titles = [item['raw_text'] for item in batch_data]
    
    for idx, cid in enumerate(cluster_assignments):
        uid = batch_data[idx]['id']
        title = batch_data[idx]['raw_text'][:30] + "..."
        print(f"  Item {uid} ({title}) -> Cluster {cid.item()}")
        
    # å¯è§†åŒ–
    viz_path = os.path.join(base_dir, "cluster_viz.png")
    visualize_clusters(all_base_features, cluster_assignments, item_ids, all_raw_titles, save_path=viz_path)
        
    # 6. é˜¶æ®µ 3: ååŒè¯­ä¹‰å¢å¼º (Final Representation)
    print("\n[é˜¶æ®µ 3: ååŒè¯­ä¹‰å¢å¼º (M3CSR Final Inference)]")
    
    for idx, item in enumerate(batch_data):
        uid = item['id']
        cid = cluster_assignments[idx].unsqueeze(0) # [1]
        
        with torch.no_grad():
            final_vec, weights = model(
                item['video'], 
                item['audio'], 
                item['text'], 
                cid, 
                cover=item['cover']
            )
            
        w = weights.tolist()[0]
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nItem {uid} æœ€ç»ˆå¢å¼ºå‘é‡: {final_vec.shape}")
        print(f"  æ¨¡æ€æƒé‡: Video={w[0]:.3f}, Audio={w[1]:.3f}, Text={w[2]:.3f}")
        
        # ç®€å•åˆ¤å®šä¸»å¯¼æ¨¡æ€
        modes = ['Video', 'Audio', 'Text']
        max_idx = w.index(max(w))
        print(f"  ä¸»å¯¼æ¨¡æ€: {modes[max_idx]}")

if __name__ == "__main__":
    main()


'''

è¿è¡Œ: python test_real_data.py
è¾“å‡º:
============================================================
M3CSR å¤šæ¨¡æ€å¤„ç†æµç¨‹ (Batch Processing - Auto Clustering)
============================================================

[åˆå§‹åŒ–æ¨¡å‹]
INFO:multimodal_encoder_1216:M3CSR: ä½¿ç”¨ VGGishAudioEncoder
INFO:multimodal_encoder_1216:Added VGGish path: F:\coldstart-recsys\models\VGGish\hub\harritaylor_torchvggish_master
INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: F:/coldstart-recsys/models/SBERT/all-MiniLM-L6-v2
INFO:multimodal_encoder_1216:VideoEncoder å·²æˆåŠŸå†»ç»“ (Parameters frozen + Eval mode).
INFO:multimodal_encoder_1216:AudioEncoder å·²æˆåŠŸå†»ç»“ (Parameters frozen + Eval mode).
INFO:multimodal_encoder_1216:TextEncoder å·²æˆåŠŸå†»ç»“ (Parameters frozen + Eval mode).

[é˜¶æ®µ 1: æ•°æ®åŠ è½½ä¸åŸºç¡€ç‰¹å¾æå–]
æ­£åœ¨å¤„ç† Item ID: 1 ...
Batches:   0%|                                                        | 0/1 [00:00<?, ?it/s]D:\CodeTools\Anaconda\envs\torch\lib\site-packages\transformers\models\bert\modeling_bert.py:413: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.26it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 107.99it/s]
æ­£åœ¨å¤„ç† Item ID: 2 ...
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 147.95it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 128.70it/s]
æ­£åœ¨å¤„ç† Item ID: 3 ...
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 166.41it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 166.20it/s] 
æ­£åœ¨å¤„ç† Item ID: 4 ...
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 98.00it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 120.09it/s] 
æ­£åœ¨å¤„ç† Item ID: 5 ...
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 100.45it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 113.95it/s] 
æ­£åœ¨å¤„ç† Item ID: 6 ...
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 126.95it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 89.10it/s] 
æ­£åœ¨å¤„ç† Item ID: 7 ...
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 156.05it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 117.15it/s] 

æ‰€æœ‰ Item åŸºç¡€ç‰¹å¾æå–å®Œæ¯•: torch.Size([7, 256])

[é˜¶æ®µ 2: å…¨å±€è¯­ä¹‰èšç±» (æ¨¡æ‹Ÿ Offline è¿‡ç¨‹)]
è‡ªåŠ¨å¯»æ‰¾æœ€ä½³ K å€¼ (èŒƒå›´ 2-6)...
  k=2: Silhouette Score = 0.1800
  k=3: Silhouette Score = 0.1753
  k=4: Silhouette Score = 0.1317
  k=5: Silhouette Score = 0.0420
  k=6: Silhouette Score = 0.0048
âœ… é€‰å®šæœ€ä½³ K å€¼: 2 (Score: 0.1800)
èšç±»ç»“æœ:
  Item 1 (ğŸ§ ASMR Cats Grooming  #asmr #A...) -> Cluster 0
  Item 2 (Rain Sound On Window with Thun...) -> Cluster 1
  Item 3 (Relaxing Snowfall - Sound of L...) -> Cluster 1
  Item 4 (This is a text document for te...) -> Cluster 1
  Item 5 (Dare you to laugh?! #cat #kitt...) -> Cluster 0
  Item 6 (Oh no, fallen into a human tra...) -> Cluster 0
  Item 7 (Sonar Cat #cat #cute #kitty #k...) -> Cluster 0

[Visualization] èšç±»åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: f:\coldstart-recsys\models\mutibehavior\test_file\cluster_viz.png

[é˜¶æ®µ 3: ååŒè¯­ä¹‰å¢å¼º (M3CSR Final Inference)]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.80it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 117.76it/s]

Item 1 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.460, Audio=0.288, Text=0.252
  ä¸»å¯¼æ¨¡æ€: Video
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 119.98it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 100.63it/s]

Item 2 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.284, Audio=0.235, Text=0.481
  ä¸»å¯¼æ¨¡æ€: Text
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 117.56it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 116.54it/s]

Item 3 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.283, Audio=0.234, Text=0.483
  ä¸»å¯¼æ¨¡æ€: Text
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 133.23it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 122.72it/s]

Item 4 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.281, Audio=0.236, Text=0.483
  ä¸»å¯¼æ¨¡æ€: Text
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 122.37it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 114.04it/s] 

Item 5 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.464, Audio=0.287, Text=0.249
  ä¸»å¯¼æ¨¡æ€: Video
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 112.58it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 82.78it/s]

Item 6 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.465, Audio=0.286, Text=0.249
  ä¸»å¯¼æ¨¡æ€: Video
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 139.55it/s]
Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 137.27it/s] 

Item 7 æœ€ç»ˆå¢å¼ºå‘é‡: torch.Size([1, 256])
  æ¨¡æ€æƒé‡: Video=0.464, Audio=0.287, Text=0.249
  ä¸»å¯¼æ¨¡æ€: Video

'''